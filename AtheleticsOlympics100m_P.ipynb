{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeMzcMbv-PqP"
      },
      "source": [
        "## **SETUP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uPxauH1Xwy_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad37a14-f7c6-4acc-ff16-8e25f8fdff02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
        "# SOME COMMON OPTIONS AND LIBRARIES THAT I USE  = = = = = = = = = = = = = \n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
        "\n",
        "import urllib.request\n",
        "# pandas para manipulação de dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
        "\n",
        "# upload de arquivos \n",
        "from google.colab import files\n",
        "\n",
        "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
        "# Configurando o número máximo de linhas a se mostrar\n",
        "pd.set_option('display.max_row', 5000)\n",
        "\n",
        "# Configurando o número máximo de colunas a se mostrar\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "# Aumentando o número de caracteres a serem exibidos numa coluna de texto\n",
        "pd.options.display.max_colwidth = 500\n",
        "\n",
        "# Desligando a notificação setcopywarning\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "import datetime as DT\n",
        "import io\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import googleapiclient.discovery\n",
        "import numpy as np \n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from pandas.io.json import json_normalize\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from urllib.request import urlopen \n",
        "from bs4 import BeautifulSoup \n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m_ybu-HKnaD"
      },
      "source": [
        "# **RETRIEVE ALL LINKS FOR ATHLETICS IN SUMMER OLYMPICS PAGE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxtp1LqDIqDs"
      },
      "outputs": [],
      "source": [
        "###########################################################################################################\n",
        "###\n",
        "########## RETRIEVE ALL LINKS FOR \"ATHLETICS IN SUMMER OLYMPICS\" ON WIKIPEDIA\n",
        "###\n",
        "###########################################################################################################\n",
        "\n",
        "res = requests.get(\"https://en.wikipedia.org/wiki/Athletics_at_the_Summer_Olympics\")\n",
        "soup = bs(res.text, \"html.parser\")\n",
        "links = soup.select('a')\n",
        "type(links)\n",
        "\n",
        "athletic_links = []\n",
        "for link in links:\n",
        "    url = link.get(\"href\", \"\")\n",
        "    if \"/wiki/Athletics\" in url:\n",
        "\n",
        "        d = {'url': url}         \n",
        "        ooo = pd.DataFrame(d, index=[0])\n",
        "\n",
        "        athletic_links.append(ooo) \n",
        "\n",
        "df_athletics = pd.concat(athletic_links).reset_index()\n",
        "df_athletics\n",
        "\n",
        "# NOW IT'S TIME TO FILTER AND RETAIN ONLY THOSE LINKS REGARDING OLYMPIC EDITIONS\n",
        "\n",
        "df_athletics['link']     = df_athletics['url'].str.extract(r'(/wiki/Athletics_at_the_\\d\\d\\d\\d_Summer_Olympics_)*')\n",
        "df_athletics2 = df_athletics.dropna(subset=['link']).reset_index()\n",
        "df_athletics2 = df_athletics2[['url']]\n",
        "df_athletics2.head(20)\n",
        "\n",
        "# SOME REGEX CLEANNING\n",
        "\n",
        "import re\n",
        "df_athletics2['year']=df_athletics2['url'].str.extract(r'(\\d{4})')\n",
        "df_athletics2['year'] = df_athletics2['year'].astype(int)\n",
        "df_athletics2.sort_values(['year'], ascending=[True], inplace=True)\n",
        "df_athletics2 = df_athletics2.drop_duplicates()\n",
        "#df_athletics2['type'] = df_athletics2['url'].str.extract(r'(?<=27s_).*')\n",
        "df_athletics2['event'] = df_athletics2['url'].str.extract(r'(27s_[0-9A-za-z%,]+|Mixed_[0-9A-za-z%,]+).*')\n",
        "df_athletics2['event'] = df_athletics2['event'].str.replace(\"27s_\",\"\")\n",
        "df_athletics2['event'] = df_athletics2['event'].str.replace(\"%C3%97\",\"x\")\n",
        "df_athletics2['event'] = df_athletics2['event'].str.replace(\"_\",\" \")\n",
        "df_athletics2['men_women'] = df_athletics2['url'].str.extract(r'(Women|Men)') \n",
        "\n",
        "# DATAFRAME LINKS FOR EVERY EVENT FROM 1896 TO 2021, MAN AND WOMAN\n",
        "df_athletics2.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A LOOK AT THE 100M"
      ],
      "metadata": {
        "id": "7Q43W6juaiXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_athletics3 = df_athletics2.query('event == \"100 metres\"')\n",
        "df_athletics3"
      ],
      "metadata": {
        "id": "RNzIK7HJsQo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BEAUTIFUL SOUP**"
      ],
      "metadata": {
        "id": "isj1tI9laKit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "olympic_games = df_athletics2.query('event == \"100 metres\"')\n",
        "olympic_games\n",
        "\n",
        "sleep_min = 1\n",
        "sleep_max = 3\n",
        "start_time = time.time()\n",
        "\n",
        "table_bodies  = []\n",
        "heats_column0 = []\n",
        "all_tables0 = []\n",
        "headers = []\n",
        "table_titles = []\n",
        "\n",
        "for olympics in olympic_games.itertuples():\n",
        "        \n",
        "    url0 = getattr(olympics, 'url') \n",
        "    url  = 'https://en.wikipedia.org/'+url0\n",
        "    #print(url)\n",
        "    \n",
        "    year = getattr(olympics, 'year')\n",
        "    print(year)\n",
        "    men_women = getattr(olympics, 'men_women')    \n",
        "    event = getattr(olympics, 'event')\n",
        "\n",
        "    page = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "    time.sleep(np.random.uniform(sleep_min, sleep_max))\n",
        "\n",
        "    # ===========================================================================================================\n",
        "    # 1 - TAKE EVERYTHING THAT'S BETWEEN \"RESULTS\" AND \"FINAL\" TABLE. THAT REPRESENTS WHAT WE WANT FROM THE PAGE\n",
        "    # ===========================================================================================================\n",
        "\n",
        "    heats = soup.find_all('span', {'class': 'mw-headline'})\n",
        "    for i in heats:\n",
        "      t = i.find_previous()\n",
        "      h = t.find('span', {'class': 'mw-headline'}).text\n",
        "      d = {'table_name': h, 'year': year, 'men_women': men_women}         \n",
        "      ooo = pd.DataFrame(d, index=[0])\n",
        "  \n",
        "      table_titles.append(ooo)\n",
        "      \n",
        "    table_titles_final = pd.concat(table_titles).reset_index().drop(['index'], axis=1).reset_index()\n",
        "    table_titles_final.rename(columns={\"index\": \"id\"}, inplace=True)\n",
        "     \n",
        "    table_titles_final['min_val'] = np.where( ((table_titles_final['table_name'] == \"Results\") | (table_titles_final['table_name'] == \"Heats\")), 1, np.nan)\n",
        "    table_titles_final['max_val'] = np.where( ((table_titles_final['table_name'] == \"Final\")), 2,np.nan)\n",
        "    table_titles_final['val'] = table_titles_final['min_val'].combine_first(table_titles_final['max_val']).fillna(0)#.astype(np.int64)\n",
        "    table_titles_final = table_titles_final.drop(['min_val','max_val'], axis=1)\n",
        "    table_titles_final\n",
        "\n",
        "    # ==================================================================================\n",
        "    # 2 - TAKE EVERY TABLE HEADER FROM ALL TABLES WITH \"MW-HEADLINE\"\n",
        "    # ==================================================================================\n",
        "\n",
        "    table_data = soup.find_all('table', {\"class\":[\"wikitable sortable\", \"wikitable\"]})\n",
        "    #print(len(table_data))\n",
        "\n",
        "    # FOR EVERY TABLE IN MY TABLE DATA, I WANT THE \"TH\" TAG, IE., THE HEADER\n",
        "    for i in table_data:\n",
        "        th = i.find_all('th')\n",
        "\n",
        "        # THEN I RETRIEVE THE TEXT FROM THE PREVIOUS SPAN TAG WITH MW-HEADLINE CLASS\n",
        "\n",
        "        table_name = i.find_previous('span', {'class': 'mw-headline'}).text\n",
        "        #print(table_name)\n",
        "        count = 0\n",
        "        \n",
        "        for j in th:\n",
        "            if len(j.text.strip()) <= 1:              \n",
        "              th2 = \"apagar\"\n",
        "              #print(\"vazio\", len(j), j)\n",
        "            else:\n",
        "              th2 = j.text.strip()\n",
        "              count = count+1\n",
        "              #print(len(j), j)\n",
        "            d = {'table_name': table_name, 'column_order_id': count, 'column_name': th2, 'year': year, 'men_women': men_women}         \n",
        "            ooo = pd.DataFrame(d, index=[0])\n",
        "            headers.append(ooo)\n",
        "\n",
        "headers_final = pd.concat(headers).reset_index().drop(['index'], axis=1).reset_index()\n",
        "\n",
        "# DELETING \"OVERALL\" HEADERS\n",
        "headers_final = headers_final[ headers_final['table_name'].str.find(\"Overall\") == -1 ]\n",
        "\n",
        "# CLEANING TABLE NAMES THAT CONTAINS THE DOT \".\"\n",
        "headers_final['table_name'] = headers_final['table_name'].str.replace(\".\",\"\")\n",
        "headers_final['column_name'] = headers_final['column_name'].str.replace(\".\",\"\")\n",
        "headers_final['column_name'] = headers_final['column_name'].str.replace(\"Country\",\"Nation\")\n",
        "headers_final\n",
        "\n",
        "table_titles_final['concat_for'] = table_titles_final['year'].astype(str) + table_titles_final['men_women']\n",
        "table_titles_final\n",
        "\n",
        "year_genre_loop = table_titles_final['concat_for'].unique().T\n",
        "year_genre_loop = pd.DataFrame(year_genre_loop)\n",
        "year_genre_loop.rename(columns={0: \"year_genre\"}, inplace=True)\n",
        "year_genre_loop\n",
        "\n",
        "right_tables = []\n",
        "\n",
        "for ygenre in year_genre_loop.itertuples():\n",
        "    \n",
        "    filter = getattr(ygenre, 'year_genre')\n",
        "\n",
        "    table_titles_final2 = table_titles_final.query('concat_for == @filter ')\n",
        "    print(table_titles_final2)\n",
        "\n",
        "    year = getattr(table_titles_final2, 'year')\n",
        "    men_women = getattr(table_titles_final2, 'men_women')    \n",
        "    val = getattr(table_titles_final2, 'val')    \n",
        "    id = getattr(table_titles_final2, 'id')\n",
        "    table_name = getattr(table_titles_final2, 'table_name')    \n",
        "\n",
        "    if len(table_titles_final2.query('table_name==\"Results\"')) > 0:\n",
        "      start = \"Results\"\n",
        "    else:\n",
        "      start = \"Heats\"\n",
        "\n",
        "    df_min = table_titles_final2.query('table_name==@start')\n",
        "    min_value = df_min['id'].values[0]\n",
        "    min_value\n",
        "\n",
        "    df_max = table_titles_final2.query('table_name==\"Final\"')\n",
        "    max_value = df_max['id'].values[0]\n",
        "    max_value\n",
        "\n",
        "    table_titles_final3 = table_titles_final2.query('id >= @min_value & id<= @max_value ')\n",
        "    right_tables.append(table_titles_final3)\n",
        "\n",
        "tables_to_extract = pd.concat(right_tables)\n",
        "tables_to_extract\n",
        "\n",
        "table_scope0 = pd.merge(tables_to_extract, headers_final, left_on=['table_name', 'year', 'men_women'], right_on=['table_name', 'year', 'men_women'], how='inner').drop(['id', 'index'], axis=1)\n",
        "table_scope = table_scope0[table_scope0.column_name != \"apagar\"]\n",
        "table_scope.sort_index()\n",
        "table_scope\n",
        "\n",
        "table_scope = table_scope.drop(['val','concat_for'], axis=1)\n",
        "table_scope1 = table_scope[['year']].drop_duplicates()\n",
        "\n",
        "table_append=[]\n",
        "\n",
        "for row in table_scope1.itertuples():\n",
        "    #print(row)\n",
        "    year = getattr(row, 'year')\n",
        "\n",
        "    filtered = table_scope1.query('year == @year')\n",
        "\n",
        "    for row2 in filtered.itertuples():\n",
        "      filtered_year = table_scope[['table_name','year','men_women']].query('year == @year')\n",
        "      unique_combination = filtered_year.drop_duplicates()\n",
        "      \n",
        "      filtered_genre = unique_combination[['men_women']].drop_duplicates()\n",
        "\n",
        "      for row3 in filtered_genre.itertuples():\n",
        "\n",
        "        men_women = getattr(row3, 'men_women')\n",
        "\n",
        "        genre = unique_combination.query('men_women == @men_women')\n",
        "\n",
        "        #print(unique_combination)\n",
        "        genre['id'] = 1\n",
        "        genre['idx'] = genre.id.cumsum()\n",
        "        table_append.append(genre)\n",
        "\n",
        "append_final = pd.concat(table_append)\n",
        "append_final\n",
        "\n",
        "'''\n",
        "table_scope.to_csv('/content/drive/My Drive/headers_final.csv')\n",
        "table_titles_final2.to_csv('/content/drive/My Drive/table_titles.csv')\n",
        "'''\n"
      ],
      "metadata": {
        "id": "dzGPuMcvaLqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxB_Jg6EOffU"
      },
      "source": [
        "# **PANDAS HTML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kyzW_xFOfHu"
      },
      "outputs": [],
      "source": [
        "olympic_games = df_athletics2.query('event == \"100 metres\"')\n",
        "olympic_games\n",
        "\n",
        "sleep_min = 1\n",
        "sleep_max = 3\n",
        "start_time = time.time()\n",
        "\n",
        "table_data  = []\n",
        "\n",
        "for olympics in olympic_games.itertuples():\n",
        "    \n",
        "    url0 = getattr(olympics, 'url') \n",
        "    url  = 'https://en.wikipedia.org/'+url0\n",
        "    #print(url)\n",
        "    \n",
        "    year = getattr(olympics, 'year')\n",
        "    #print(year)\n",
        "    \n",
        "    men_women = getattr(olympics, 'men_women')    \n",
        "    event = getattr(olympics, 'event')\n",
        "\n",
        "    page = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "    time.sleep(np.random.uniform(sleep_min, sleep_max))\n",
        "                 \n",
        "    table_MN = pd.read_html(url, match='Rank', attrs={'class': 'wikitable sortable'})\n",
        "    #print(  len(table_MN), range(len(table_MN)), range(table_MN)  )\n",
        "    for i in range(len(table_MN)):\n",
        "        #print(i)\n",
        "        #print(table_MN[i])\n",
        "\n",
        "        for j in table_MN[i]:\n",
        "          table_MN[i]['idx'] = i+1\n",
        "          table_MN[i]['year'] = year\n",
        "          table_MN[i]['men_women'] = men_women\n",
        "          table_MN[i]['event'] = event\n",
        "\n",
        "        table_data.append(table_MN[i])\n",
        "\n",
        "df_extract = pd.concat(table_data)\n",
        "df_extract       \n",
        "\n",
        "print(len(df_extract))\n",
        "\n",
        "base_final_trat0 = pd.merge(df_extract, append_final, left_on=['idx', 'year', 'men_women'], right_on=['idx', 'year', 'men_women'], how='inner', indicator=True)\n",
        "base_final_trat0\n",
        "\n",
        "# WHEN WE FILTER THE TABLE SCOPE TO CONTAIN ONLY THE COLUMNS YEAR, MEN_WOMEN AND TABLE_NAME, REMOVING THE DUPLICATES, WE GET A TABLE THAT CAN BE\n",
        "# USED AS A FILTER TO PREVENT CERTAIN TABLES FROM GENERATING BLANK ROWS. FOR EXAMPLE, 1896 AND 1904 THE RESULTS SUMMARY TABLE APPEARS AS IDX5,\n",
        "# BUT SHOULDN'T BE THERE. AS IN BASE_FINAL_TRAT 1 WE ALREADY HAVE THE TABLE_NAME, IF WE MAKE AN INNER JOIN \"FILTER\" ONLY THE VALID STEPS (TABLES)\n",
        "# FOR EACH OF THESE COMPETITIONS\n",
        "\n",
        "# FILTERING THE COLUMNS \n",
        "table_scope_filter = table_scope[['year','men_women','table_name']].drop_duplicates()\n",
        "table_scope_filter\n",
        "\n",
        "base_final_trat1 = base_final_trat0[['year'\t,'men_women','table_name', 'Rank',\t'Athlete'\t,'Nation',\t'Time',\t'Notes'\t,'idx'\t,\t'event']]\n",
        "\n",
        "print(len(base_final_trat1))\n",
        "print(len(table_scope_filter))\n",
        "base_final_trat2 = pd.merge(base_final_trat1, table_scope_filter, left_on=['table_name', 'year', 'men_women'], right_on=['table_name', 'year', 'men_women'], how='inner', indicator=True)\n",
        "base_final_trat2\n",
        "print(len(base_final_trat2))\n",
        "\n",
        "# FINAL TABLE:\n",
        "\n",
        "base_final_trat2 = base_final_trat2.drop(['_merge'], axis=1)\n",
        "base_right = pd.merge(base_final_trat2, df_extract, left_on=['idx', 'year', 'men_women'], right_on=['idx', 'year', 'men_women'], how='outer', indicator=True).query('_merge == \"right_only\"')\n",
        "base_right.head(5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AeMzcMbv-PqP"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMIVI/MrrR3OYQ9JA3Rx1JD"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}